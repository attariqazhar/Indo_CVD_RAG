{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseHTML(url):\n",
    "    # Send GET request to url with headers\n",
    "    load_dotenv(override=True)\n",
    "    user_email = os.getenv('SCRAPING_USER_EMAIL')\n",
    "    headers = {'user-agent' : 'Mozilla/5.0 (X11; Linux x86_64); {user_email}'}\n",
    "    req = requests.get(url, headers = headers).text\n",
    "\n",
    "    # Parse HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    return soup\n",
    "\n",
    "def get_url(href):\n",
    "    if href[0] != '/':\n",
    "        href = '/' + href\n",
    "    return 'https://www.alodokter.com' + href\n",
    "\n",
    "def get_cvd_page():\n",
    "    html_parsed = parseHTML(get_url('jantung'))\n",
    "    # retrieve class menu-children from html\n",
    "    parse = html_parsed.find('ul', class_='menu-children')\n",
    "    return parse\n",
    "\n",
    "def get_topic_page_with_disease_container(url):\n",
    "    html_parsed = parseHTML(url)\n",
    "    topic_title = html_parsed.find('div', class_='title-tag-container').find('h2').text\n",
    "    subtopics = []\n",
    "    tag_list = html_parsed.find('div', class_='disease-tag-container').find_all('a')\n",
    "\n",
    "    for tag in tag_list:\n",
    "        \n",
    "        if tag.div.text.strip() != 'Pengertian':\n",
    "            subtopic_title = tag.div.text.strip() \n",
    "            subtopic_url = get_url(tag['href'])\n",
    "            subtopic_content = get_subtopic_content(subtopic_url)\n",
    "            for topic in subtopic_content:\n",
    "                subtopics.append(topic)\n",
    "        else: \n",
    "            # First Subtopic Title\n",
    "            subtopic_title = html_parsed.find('div', class_='title-tag-container').h1.text.strip()\n",
    "\n",
    "            content_parse = html_parsed.find('div', class_='post-content')\n",
    "            subtopic_content = ''\n",
    "            for parse in content_parse.find_all():\n",
    "                if parse.name == 'h3':\n",
    "                    break\n",
    "                if parse.name == 'p':\n",
    "                    subtopic_content += parse.text.strip() + '\\n'\n",
    "                elif parse.name == 'ul':\n",
    "                    for li in parse.find_all('li'):\n",
    "                        subtopic_content += '- ' + li.text.lstrip('\\n') + '\\n'\n",
    "\n",
    "            subtopic = {\n",
    "                \"subtopic\": subtopic_title,\n",
    "                \"subtopic_url\": url,\n",
    "                \"content\": subtopic_content\n",
    "            }\n",
    "\n",
    "            subtopics.append(subtopic)\n",
    "    \n",
    "    topic = {\n",
    "        \"topic\": topic_title,\n",
    "        \"url\" : url,\n",
    "        \"subtopics\" : subtopics\n",
    "    }\n",
    "\n",
    "    return topic\n",
    "\n",
    "def get_subtopic_content(url):\n",
    "    html_parsed = parseHTML(url)\n",
    "    subtopics = []\n",
    "\n",
    "    # First Subtopic Title\n",
    "    subtopic_title = html_parsed.find('div', class_='title-tag-container').h1.text.strip()\n",
    "\n",
    "    content_parse = html_parsed.find('div', class_='post-content')\n",
    "    subtopic_content = ''\n",
    "    for parse in content_parse.find_all():\n",
    "        if parse.name == 'h3' or parse.find('strong') is not None:\n",
    "            break\n",
    "        if parse.name == 'p':\n",
    "            # parse text in p and add new line\n",
    "            subtopic_content += parse.text.strip() + '\\n'\n",
    "        elif parse.name == 'ul':\n",
    "            for li in parse.find_all('li'):\n",
    "                subtopic_content += '- ' + li.text.lstrip('\\n') + '\\n'\n",
    "\n",
    "    subtopic = {\n",
    "        \"subtopic\": subtopic_title,\n",
    "        \"subtopic_url\": url,\n",
    "        \"content\": subtopic_content\n",
    "    }\n",
    "\n",
    "    subtopics.append(subtopic)\n",
    "        \n",
    "    additional_title = subtopic_title\n",
    "    # Next Subtopic Title\n",
    "    # find all component (h3, h4, or p) that contains 'strong' component inside the component\n",
    "    parent_elements = html_parsed.find_all(\n",
    "        lambda tag: tag.name in ['h3', 'h4', 'p'] and tag.find('strong')\n",
    "    )\n",
    "    \n",
    "    for parse in parent_elements:\n",
    "        # loop components after h3 until it finds new h3\n",
    "        subtopic_title = additional_title + ': ' + parse.text.strip()\n",
    "        subtopic_content = ''\n",
    "\n",
    "        content_parse = parse.find_next_sibling()\n",
    "        # while content_parse is not None and content_parse that is not li does not contain strong inside the component\n",
    "        while content_parse is not None and (content_parse.name == 'ul' or content_parse.find('strong') is None):\n",
    "            if content_parse.name == 'p':\n",
    "                subtopic_content += content_parse.text.strip() + '\\n'\n",
    "            elif content_parse.name == 'ul':\n",
    "                for li in content_parse.find_all('li'):\n",
    "                    # add thick dot chara\n",
    "                    subtopic_content += '- ' + li.text.lstrip('\\n') + '\\n'\n",
    "            content_parse = content_parse.find_next_sibling()\n",
    "\n",
    "        subtopic = {\n",
    "            \"subtopic\": subtopic_title,\n",
    "            \"subtopic_url\": url,\n",
    "            \"content\": subtopic_content,\n",
    "        }\n",
    "\n",
    "        subtopics.append(subtopic)\n",
    "\n",
    "    return subtopics\n",
    "\n",
    "def get_topic_page(url):\n",
    "    html_parsed = parseHTML(url)\n",
    "    subtopics = []\n",
    "\n",
    "    # First Subtopic Title\n",
    "    topic_title = html_parsed.find('div', class_='title-tag-container').h1.text.strip()\n",
    "    subtopic_title = \"Pengertian \" + topic_title\n",
    "\n",
    "    content_parse = html_parsed.find('div', class_='post-content')\n",
    "    subtopic_content = ''\n",
    "    for parse in content_parse.find_all():\n",
    "        if parse.name == 'h3' or parse.name == 'h4':\n",
    "            break\n",
    "        if parse.name == 'p':\n",
    "            # parse text in p and add new line\n",
    "            subtopic_content += parse.text.strip() + '\\n'\n",
    "        elif parse.name == 'ul':\n",
    "            for li in parse.find_all('li'):\n",
    "                subtopic_content += '- ' + li.text.lstrip('\\n') + '\\n'\n",
    "\n",
    "    subtopic = {\n",
    "        \"subtopic\": subtopic_title,\n",
    "        \"subtopic_url\": url,\n",
    "        \"content\": subtopic_content\n",
    "    }\n",
    "\n",
    "    subtopics.append(subtopic)\n",
    "    \n",
    "    parent_elements = html_parsed.find_all(\n",
    "        lambda tag: tag.name in ['h3', 'h4', 'p'] and (tag.find('strong') or tag.find('b'))\n",
    "    )\n",
    "    additional_title = ''\n",
    "\n",
    "    # Next Subtopic Title\n",
    "    for parse in parent_elements:\n",
    "        # loop components after h3 until it finds new h3\n",
    "        if parse.name == 'h3':\n",
    "            subtopic_title = parse.text.strip()\n",
    "            additional_title = subtopic_title.strip()\n",
    "        elif parse.name == 'h4':\n",
    "            subtopic_title = additional_title + ': ' + parse.text.strip()\n",
    "        subtopic_content = ''\n",
    "\n",
    "        content_parse = parse.find_next_sibling()\n",
    "        while content_parse is not None and (content_parse.name == 'ul' or (content_parse.find('strong') is None and content_parse.find('b') is None)) and content_parse.name != 'h3':\n",
    "            if content_parse.name == 'p':\n",
    "                subtopic_content += content_parse.text.strip() + '\\n'\n",
    "            elif content_parse.name == 'ul':\n",
    "                for li in content_parse.find_all('li'):\n",
    "                    subtopic_content += '- ' + li.text.lstrip('\\n') + '\\n'\n",
    "            content_parse = content_parse.find_next_sibling()\n",
    "\n",
    "        subtopic = {\n",
    "            \"subtopic\": subtopic_title,\n",
    "            \"subtopic_url\": url,\n",
    "            \"content\": subtopic_content,\n",
    "        }\n",
    "\n",
    "        subtopics.append(subtopic)\n",
    "    \n",
    "    topic = {\n",
    "        \"topic\": topic_title,\n",
    "        \"url\" : url,\n",
    "        \"subtopics\" : subtopics\n",
    "    }\n",
    "\n",
    "    return topic\n",
    "\n",
    "def get_topic_content(url):\n",
    "    html = parseHTML(url)\n",
    "    if html.find('div', class_='disease-container') is not None:\n",
    "        return get_topic_page_with_disease_container(url)\n",
    "    else:\n",
    "        return get_topic_page(url)\n",
    "\n",
    "def get_all_content():\n",
    "    topic_data = []\n",
    "    html = get_cvd_page()\n",
    "    for parse in html.find_all('li', class_='index-item'):\n",
    "        href = parse.find('a')['href']\n",
    "        topic_data.append(get_topic_content(get_url(href)))\n",
    "    return topic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all content\n",
    "data = get_all_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of topics to JSON\n",
    "with open('../data/cvd.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
