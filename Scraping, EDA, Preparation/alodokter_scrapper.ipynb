{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseHTML(url):\n",
    "    '''\n",
    "    Args: \n",
    "        url (str): URL to be parsed\n",
    "\n",
    "    Returns:\n",
    "        soup (BeautifulSoup): Parsed HTML object\n",
    "    '''\n",
    "\n",
    "    # Mengirimkan GET request ke URL\n",
    "    load_dotenv(override=True)\n",
    "    user_email = os.getenv('SCRAPING_USER_EMAIL')\n",
    "    headers = {'user-agent' : 'Mozilla/5.0 (X11; Linux x86_64); {user_email}'}\n",
    "    req = requests.get(url, headers = headers).text\n",
    "\n",
    "    # Melakukan parsing HTML dengan BeautifulSoup\n",
    "    soup = BeautifulSoup(req, \"lxml\")\n",
    "\n",
    "    return soup\n",
    "\n",
    "def get_url(href):\n",
    "    '''\n",
    "    Args:\n",
    "        href (str): Alodokter URL\n",
    "    \n",
    "    Returns:\n",
    "        str: Alodokter link to certain page\n",
    "    '''\n",
    "\n",
    "    if href[0] != '/':\n",
    "        href = '/' + href\n",
    "    return 'https://www.alodokter.com' + href\n",
    "\n",
    "def get_cvd_page():\n",
    "    '''\n",
    "    Args:\n",
    "        None\n",
    "    \n",
    "    Returns:\n",
    "        html_parsed (BeautifulSoup): Parsed HTML object of Alodokter Cardiovascular Disease Topics\n",
    "    ''' \n",
    "\n",
    "    # Mengambil halaman utama kumpulan artikel penyakit jantung dari Alodokter (https://www.alodokter.com/jantung)\n",
    "    html_parsed = parseHTML(get_url('jantung'))\n",
    "\n",
    "    # Mengambil seluruh topik penyakit jantung\n",
    "    parse = html_parsed.find('ul', class_='menu-children')\n",
    "\n",
    "    return parse\n",
    "\n",
    "# Pada artikel tentang penyakit jantung di Aldokter, terdapat dua jenis halaman:\n",
    "# 1. Halaman yang memiliki subtopik yang terpisah setiap page-nya  \n",
    "# 2. Halaman yang tidak memiliki kontainer penyakit (disease-container) dan tidak memiliki subtopik\n",
    "\n",
    "def get_topic_page_with_disease_container(url):\n",
    "    '''\n",
    "    Args:\n",
    "        url (str): URL to be parsed\n",
    "    \n",
    "    Returns:\n",
    "        topic (dict): Dictionary containing topic title, url, and subtopics. \n",
    "                        Subtopics is a list of dictionaries containing subtopic title, url, and content.\n",
    "    '''\n",
    "\n",
    "    # Fungsi ini digunakan untuk mendapatkan konten dari laman artikel yang setiap subtopiknya terpisah dalam halaman yang berbeda\n",
    "    # contoh: https://www.alodokter.com/hipertensi\n",
    "\n",
    "    html_parsed = parseHTML(url)\n",
    "    topic_title = html_parsed.find('div', class_='title-tag-container').find('h2').text\n",
    "    subtopics = []\n",
    "    tag_list = html_parsed.find('div', class_='disease-tag-container').find_all('a')\n",
    "\n",
    "    for tag in tag_list:\n",
    "        if tag.div.text.strip() != 'Pengertian':\n",
    "            # Mengambil konten dari subtopik di halaman kedua dan seterusnya\n",
    "            subtopic_title = tag.div.text.strip() \n",
    "            subtopic_url = get_url(tag['href'])\n",
    "            subtopic_content = get_subtopic_content(subtopic_url)\n",
    "            for topic in subtopic_content:\n",
    "                subtopics.append(topic)\n",
    "        else: \n",
    "            # Mengambil konten dari halaman utama (Pengertian)\n",
    "            subtopic_title = html_parsed.find('div', class_='title-tag-container').h1.text.strip()\n",
    "\n",
    "            content_parse = html_parsed.find('div', class_='post-content')\n",
    "            subtopic_content = ''\n",
    "            for parse in content_parse.find_all():\n",
    "                if parse.name == 'h3':\n",
    "                    break\n",
    "                if parse.name == 'p':\n",
    "                    subtopic_content += parse.text.strip() + '\\n'\n",
    "                elif parse.name == 'ul':\n",
    "                    for li in parse.find_all('li'):\n",
    "                        subtopic_content += '- ' + li.text.lstrip('\\n') + '\\n'\n",
    "\n",
    "            subtopic = {\n",
    "                \"subtopic\": subtopic_title,\n",
    "                \"subtopic_url\": url,\n",
    "                \"content\": subtopic_content\n",
    "            }\n",
    "\n",
    "            subtopics.append(subtopic)\n",
    "    \n",
    "    topic = {\n",
    "        \"topic\": topic_title,\n",
    "        \"url\" : url,\n",
    "        \"subtopics\" : subtopics\n",
    "    }\n",
    "\n",
    "    return topic\n",
    "\n",
    "def get_subtopic_content(url):\n",
    "    '''\n",
    "    Args:\n",
    "        url (str): URL to be parsed\n",
    "    \n",
    "    Returns:\n",
    "        subtopics (list): List of dictionaries containing subtopic title, url, and content.\n",
    "    '''\n",
    "\n",
    "    # Fungsi ini digunakan untuk mendapatkan konten dari subtopik di halaman pertama\n",
    "    # contoh: https://www.alodokter.com/hipertensi\n",
    "\n",
    "    html_parsed = parseHTML(url)\n",
    "    subtopics = []\n",
    "\n",
    "    # Mengambil konten teratas dari subtopik di halaman pertama\n",
    "    subtopic_title = html_parsed.find('div', class_='title-tag-container').h1.text.strip()\n",
    "\n",
    "    content_parse = html_parsed.find('div', class_='post-content')\n",
    "    subtopic_content = ''\n",
    "    for parse in content_parse.find_all():\n",
    "        if parse.name == 'h3' or parse.find('strong') is not None:\n",
    "            # Pada halaman pengertian, jika ada elemen h3 atau strong, maka itu adalah subtopik baru\n",
    "            break\n",
    "        if parse.name == 'p':\n",
    "            subtopic_content += parse.text.strip() + '\\n'\n",
    "        elif parse.name == 'ul':\n",
    "            for li in parse.find_all('li'):\n",
    "                subtopic_content += '- ' + li.text.lstrip('\\n') + '\\n'\n",
    "\n",
    "    subtopic = {\n",
    "        \"subtopic\": subtopic_title,\n",
    "        \"subtopic_url\": url,\n",
    "        \"content\": subtopic_content\n",
    "    }\n",
    "\n",
    "    subtopics.append(subtopic)\n",
    "        \n",
    "    # Mengambil konten dari subtopik tambahan (selain pengertian) di halaman pertama\n",
    "    additional_title = subtopic_title\n",
    "    parent_elements = html_parsed.find_all(\n",
    "        lambda tag: tag.name in ['h3', 'h4', 'p'] and tag.find('strong')\n",
    "    )\n",
    "    \n",
    "    for parse in parent_elements:\n",
    "        subtopic_title = additional_title + ': ' + parse.text.strip()\n",
    "        subtopic_content = ''\n",
    "\n",
    "        content_parse = parse.find_next_sibling()\n",
    "        while content_parse is not None and (content_parse.name == 'ul' or content_parse.find('strong') is None):\n",
    "            if content_parse.name == 'p':\n",
    "                subtopic_content += content_parse.text.strip() + '\\n'\n",
    "            elif content_parse.name == 'ul':\n",
    "                for li in content_parse.find_all('li'):\n",
    "                    subtopic_content += '- ' + li.text.lstrip('\\n') + '\\n'\n",
    "            content_parse = content_parse.find_next_sibling()\n",
    "\n",
    "        subtopic = {\n",
    "            \"subtopic\": subtopic_title,\n",
    "            \"subtopic_url\": url,\n",
    "            \"content\": subtopic_content,\n",
    "        }\n",
    "\n",
    "        subtopics.append(subtopic)\n",
    "\n",
    "    return subtopics\n",
    "\n",
    "def get_topic_page(url):\n",
    "    '''\n",
    "    Args:\n",
    "        url (str): URL to be parsed\n",
    "\n",
    "    Returns:\n",
    "        topic (dict): Dictionary containing topic title, url, and subtopics. \n",
    "                        Subtopics is a list of dictionaries containing subtopic title, url, and content.\n",
    "    '''\n",
    "\n",
    "    # Fungsi ini digunakan untuk mendapatkan konten dari laman artikel yang setiap subtopiknya berada di satu halaman yang sama\n",
    "    # contoh: https://www.alodokter.com/lemah-jantung\n",
    "    \n",
    "    html_parsed = parseHTML(url)\n",
    "    subtopics = []\n",
    "\n",
    "    # Mengambil konten teratas dari subtopik di halaman pertama\n",
    "    topic_title = html_parsed.find('div', class_='title-tag-container').h1.text.strip()\n",
    "    subtopic_title = \"Pengertian \" + topic_title\n",
    "\n",
    "    content_parse = html_parsed.find('div', class_='post-content')\n",
    "    subtopic_content = ''\n",
    "    for parse in content_parse.find_all():\n",
    "        if parse.name == 'h3' or parse.name == 'h4':\n",
    "            break\n",
    "        if parse.name == 'p':\n",
    "            # parse text in p and add new line\n",
    "            subtopic_content += parse.text.strip() + '\\n'\n",
    "        elif parse.name == 'ul':\n",
    "            for li in parse.find_all('li'):\n",
    "                subtopic_content += '- ' + li.text.lstrip('\\n') + '\\n'\n",
    "\n",
    "    subtopic = {\n",
    "        \"subtopic\": subtopic_title,\n",
    "        \"subtopic_url\": url,\n",
    "        \"content\": subtopic_content\n",
    "    }\n",
    "\n",
    "    subtopics.append(subtopic)\n",
    "    \n",
    "    # Mengambil konten dari subtopik tambahan (selain pengertian) di halaman pertama\n",
    "    parent_elements = html_parsed.find_all(\n",
    "        lambda tag: tag.name in ['h3', 'h4', 'p'] and (tag.find('strong') or tag.find('b'))\n",
    "    )\n",
    "    additional_title = ''\n",
    "\n",
    "    for parse in parent_elements:\n",
    "        if parse.name == 'h3':\n",
    "            subtopic_title = parse.text.strip()\n",
    "            additional_title = subtopic_title.strip()\n",
    "        elif parse.name == 'h4':\n",
    "            subtopic_title = additional_title + ': ' + parse.text.strip()\n",
    "        subtopic_content = ''\n",
    "\n",
    "        content_parse = parse.find_next_sibling()\n",
    "        while content_parse is not None and (content_parse.name == 'ul' or (content_parse.find('strong') is None and content_parse.find('b') is None)) and content_parse.name != 'h3':\n",
    "            if content_parse.name == 'p':\n",
    "                subtopic_content += content_parse.text.strip() + '\\n'\n",
    "            elif content_parse.name == 'ul':\n",
    "                for li in content_parse.find_all('li'):\n",
    "                    subtopic_content += '- ' + li.text.lstrip('\\n') + '\\n'\n",
    "            content_parse = content_parse.find_next_sibling()\n",
    "\n",
    "        subtopic = {\n",
    "            \"subtopic\": subtopic_title,\n",
    "            \"subtopic_url\": url,\n",
    "            \"content\": subtopic_content,\n",
    "        }\n",
    "\n",
    "        subtopics.append(subtopic)\n",
    "    \n",
    "    topic = {\n",
    "        \"topic\": topic_title,\n",
    "        \"url\" : url,\n",
    "        \"subtopics\" : subtopics\n",
    "    }\n",
    "\n",
    "    return topic\n",
    "\n",
    "def get_topic_content(url):\n",
    "    ''' \n",
    "    Args:\n",
    "        url (str): URL to be parsed\n",
    "    Returns:\n",
    "        topic (dict): Dictionary containing topic title, url, and subtopics. \n",
    "                        Subtopics is a list of dictionaries containing subtopic title, url, and content.\n",
    "    '''\n",
    "\n",
    "    html = parseHTML(url)\n",
    "    if html.find('div', class_='disease-container') is not None:\n",
    "        return get_topic_page_with_disease_container(url)\n",
    "    else:\n",
    "        return get_topic_page(url)\n",
    "\n",
    "def get_all_content():\n",
    "    '''\n",
    "    Args:\n",
    "        None\n",
    "    Returns:\n",
    "        topic_data (list): List of dictionaries containing topic title, url, and subtopics. \n",
    "                            Subtopics is a list of dictionaries containing subtopic title, url, and content.\n",
    "    '''\n",
    "\n",
    "    topic_data = []\n",
    "\n",
    "    # Mengambil halaman utama kumpulan artikel penyakit jantung dari Alodokter (https://www.alodokter.com/jantung)\n",
    "    html = get_cvd_page()\n",
    "\n",
    "    num = 1\n",
    "\n",
    "    # Mengambil seluruh topik penyakit jantung beserta konten di setiap subtopiknya\n",
    "    for parse in html.find_all('li', class_='index-item'):\n",
    "        href = parse.find('a')['href']\n",
    "        print(f'[{num}/{len(html.find_all(\"li\", class_=\"index-item\"))}] {get_url(href)}')\n",
    "        topic_data.append(get_topic_content(get_url(href)))\n",
    "        num += 1\n",
    "\n",
    "    print('Scraping done!')\n",
    "    return topic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Exectuion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/45] https://www.alodokter.com/hipertensi\n",
      "[2/45] https://www.alodokter.com/penyakit-jantung\n",
      "[3/45] https://www.alodokter.com/gagal-jantung\n",
      "[4/45] https://www.alodokter.com/serangan-jantung\n",
      "[5/45] https://www.alodokter.com/hipotensi\n",
      "[6/45] https://www.alodokter.com/angin-duduk\n",
      "[7/45] https://www.alodokter.com/emboli\n",
      "[8/45] https://www.alodokter.com/aritmia\n",
      "[9/45] https://www.alodokter.com/syok-kardiogenik\n",
      "[10/45] https://www.alodokter.com/lemah-jantung\n",
      "[11/45] https://www.alodokter.com/penyakit-katup-jantung\n",
      "[12/45] https://www.alodokter.com/jantung-berdebar\n",
      "[13/45] https://www.alodokter.com/kardiomegali\n",
      "[14/45] https://www.alodokter.com/tetralogy-of-fallot\n",
      "[15/45] https://www.alodokter.com/takikardia\n",
      "[16/45] https://www.alodokter.com/iskemia\n",
      "[17/45] https://www.alodokter.com/bradikardia\n",
      "[18/45] https://www.alodokter.com/miokarditis\n",
      "[19/45] https://www.alodokter.com/hipertensi-pulmonal\n",
      "[20/45] https://www.alodokter.com/endokarditis\n",
      "[21/45] https://www.alodokter.com/patent-foramen-ovale\n",
      "[22/45] https://www.alodokter.com/sindrom-eisenmenger\n",
      "[23/45] https://www.alodokter.com/henti-jantung-mendadak\n",
      "[24/45] https://www.alodokter.com/perikarditis\n",
      "[25/45] https://www.alodokter.com/ventricular-tachycardia\n",
      "[26/45] https://www.alodokter.com/defek-septum-ventrikel\n",
      "[27/45] https://www.alodokter.com/sindrom-brugada\n",
      "[28/45] https://www.alodokter.com/hipertrofi-ventrikel-kiri\n",
      "[29/45] https://www.alodokter.com/fibrilasi-atrium\n",
      "[30/45] https://www.alodokter.com/angina-pektoris\n",
      "[31/45] https://www.alodokter.com/aterosklerosis\n",
      "[32/45] https://www.alodokter.com/arteriosklerosis\n",
      "[33/45] https://www.alodokter.com/penyakit-jantung-bawaan\n",
      "[34/45] https://www.alodokter.com/koarktasio-aorta\n",
      "[35/45] https://www.alodokter.com/tamponade-jantung\n",
      "[36/45] https://www.alodokter.com/diseksi-aorta\n",
      "[37/45] https://www.alodokter.com/efusi-perikardium\n",
      "[38/45] https://www.alodokter.com/penyakit-jantung-koroner\n",
      "[39/45] https://www.alodokter.com/takikardia-supraventrikular\n",
      "[40/45] https://www.alodokter.com/penyakit-jantung-reumatik\n",
      "[41/45] https://www.alodokter.com/torsade-de-pointes\n",
      "[42/45] https://www.alodokter.com/atrial-flutter\n",
      "[43/45] https://www.alodokter.com/atrioventricular-block\n",
      "[44/45] https://www.alodokter.com/serangan-jantung-ringan\n",
      "[45/45] https://www.alodokter.com/broken-heart-syndrome\n",
      "Scraping done!\n"
     ]
    }
   ],
   "source": [
    "# Eksekusi fungsi untuk melakukan scraping\n",
    "data = get_all_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan data ke dalam file JSON\n",
    "with open('../data/cvd.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
